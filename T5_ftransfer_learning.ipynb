{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5-ftransfer_learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPImDTXuNXNAukq7YpvmS64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54ac741b33df4657a497c5a20990d260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5662a03be19444fab76619ad47449786",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8e176b55e3d44509bf08563d814a22cc",
              "IPY_MODEL_75a74d040dd0496d8dadbe91152a4364"
            ]
          }
        },
        "5662a03be19444fab76619ad47449786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e176b55e3d44509bf08563d814a22cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e293b28a9f364f75ace7642678abd828",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6379aa1f5724141a274ff17c2e82f01"
          }
        },
        "75a74d040dd0496d8dadbe91152a4364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6630ef87ba4b471caf23a0e368ceb791",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/? [00:05&lt;00:00,  1.30s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89f558dbc0694ff489eb6b77400cb5c3"
          }
        },
        "e293b28a9f364f75ace7642678abd828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6379aa1f5724141a274ff17c2e82f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6630ef87ba4b471caf23a0e368ceb791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89f558dbc0694ff489eb6b77400cb5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jp-ishimwe/Projects/blob/master/T5_ftransfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxxkIi-xDrUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Import packages { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import progressbar\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "!pip install transformers -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LME26NCmM8ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f4d7fc18-3fce-4068-97ea-bc9f6f88f42f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rF67H7GnK2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "80c73f68-6939-4b99-f996-2f73850b6c5f"
      },
      "source": [
        "!nvidia-smi\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul  8 09:11:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    27W /  75W |   7591MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g1xKdoCnMAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e58f21b3-7fcd-4d6d-c81a-8a75f5ddc606"
      },
      "source": [
        "#@title To proceed should be cuda { run: \"auto\", vertical-output: true }\n",
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jvIoL5I0Eb5",
        "colab_type": "text"
      },
      "source": [
        "#### Getting dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGVOHhrk0GOP",
        "colab_type": "text"
      },
      "source": [
        "You can find the dataset [here](https://github.com/mahnazkoupaee/WikiHow-Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkDYkbKV0SUM",
        "colab_type": "text"
      },
      "source": [
        "**Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sAQxsN4M8k-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2697df8-3085-47f8-88e7-c40e7368c473"
      },
      "source": [
        "with open('/content/drive/My Drive/Colab Notebooks/wikihowAll.csv') as f:\n",
        "  wikihowAll = pd.read_csv(f)\n",
        "print(f'The size of data:{wikihowAll.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of data:(215365, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eia69USAvek8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "9de5cb09-297c-440d-b60e-22b33590f504"
      },
      "source": [
        "wikihowAll.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nKeep related supplies in the same area.,\\nMa...</td>\n",
              "      <td>How to Be an Organized Artist1</td>\n",
              "      <td>If you're a photographer, keep all the necess...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\nCreate a sketch in the NeoPopRealist manner ...</td>\n",
              "      <td>How to Create a Neopoprealist Art Work</td>\n",
              "      <td>See the image for how this drawing develops s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline  ...                                               text\n",
              "0  \\nKeep related supplies in the same area.,\\nMa...  ...   If you're a photographer, keep all the necess...\n",
              "1  \\nCreate a sketch in the NeoPopRealist manner ...  ...   See the image for how this drawing develops s...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX6clAc9wHuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction = {\"isn't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you ll\":\"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you re\": \"you are\" , \"you've\": \"you have\"}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E38ktgfStkI9",
        "colab_type": "text"
      },
      "source": [
        "###### Checking for NA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-NTfClirfL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e95ccecc-c584-4111-f5c2-506840dbe7d7"
      },
      "source": [
        "wikihowAll.isnull().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "headline     818\n",
              "title          1\n",
              "text        1071\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF6jYdXItg7W",
        "colab_type": "text"
      },
      "source": [
        "we have 818 articles without summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOAJwnKks87U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_wikihowAll = wikihowAll[['text', 'headline']]\n",
        "new_wikihowAll = new_wikihowAll.dropna(how='all')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGJZY7L2tTWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7f682db0-d585-4425-cbde-cbe8af68f52e"
      },
      "source": [
        "new_wikihowAll.isnull().sum()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text        253\n",
              "headline      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6BJG-DsuQ4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = np.where(new_wikihowAll['text'].isna())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ2ZXsLmuuIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "fd37ad08-4b50-4d8a-819b-f89c6550b892"
      },
      "source": [
        "new_wikihowAll.iloc[indices]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Colour coordinate your looks (For all Events)\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4328</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nRecognize the following symptoms:\\n\\nBloatin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7190</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nAt each practice, teach all of your squad di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7228</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Understand that these directions assume the re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7378</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Dribble with your left or right hand, whicheve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208605</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nStretch each arm as far across your chest wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210787</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nDownshift to a lower gear to get more power ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212574</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nThe next thing you need to do is show Invisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213977</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nIt is the same as the chorus, but with the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214419</th>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nLower the fifth 1/2 step (make the D into a ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>253 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       text                                           headline\n",
              "1492    NaN  Colour coordinate your looks (For all Events)\\n\\n\n",
              "4328    NaN  \\nRecognize the following symptoms:\\n\\nBloatin...\n",
              "7190    NaN  \\nAt each practice, teach all of your squad di...\n",
              "7228    NaN  Understand that these directions assume the re...\n",
              "7378    NaN  Dribble with your left or right hand, whicheve...\n",
              "...     ...                                                ...\n",
              "208605  NaN  \\nStretch each arm as far across your chest wh...\n",
              "210787  NaN  \\nDownshift to a lower gear to get more power ...\n",
              "212574  NaN  \\nThe next thing you need to do is show Invisi...\n",
              "213977  NaN  \\nIt is the same as the chorus, but with the s...\n",
              "214419  NaN  \\nLower the fifth 1/2 step (make the D into a ...\n",
              "\n",
              "[253 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNQP3dX3vzaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7f5dd79d-7b50-48e3-b7b5-5db0c2c062e7"
      },
      "source": [
        "new_wikihowAll = new_wikihowAll.dropna().reset_index(drop=True)\n",
        "new_wikihowAll.isnull().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text        0\n",
              "headline    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbFoCHsVv9Fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b94bf08-4ab8-4c40-925c-3170fb5920dc"
      },
      "source": [
        "new_wikihowAll.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(214294, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wBbyQwv5P1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# h = new_wikihowAll\n",
        "\n",
        "# for x in ['text', 'headline']:\n",
        "# \t\th[x+'_tokenized'] = h[x].apply(lambda x:x.lower().split() );\n",
        "# \t\th[x+'_len'] = h[x+'_tokenized'].apply(lambda x: len(x))\n",
        "# min(h['headline_len']), min(h['text_len']), len(h[h['headline_len']==1]), len(h[h['text_len']==0])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HQ2wJrXEAcr",
        "colab_type": "text"
      },
      "source": [
        "#####Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_qmLtaZp9s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextProccessing(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, column, contraction):\n",
        "    self.column = column\n",
        "    self.contraction = contraction\n",
        "  def fit(self, x, y=None):\n",
        "    return self\n",
        "  @staticmethod\n",
        "  def preprocessing(line):\n",
        "    \n",
        "    line = line.lower() \n",
        "    line = re.sub(r\"http\\S+\", \" \",line)\n",
        "    line = re.sub(r'[^\\sa-zA-Z0-9.,!?]',' ',line)\n",
        "    line = line.replace('-', ' ')\n",
        "    line = line.strip(' ')\n",
        "    line = re.sub(r'\\s{2,}',' ', line)\n",
        "\n",
        "    return line\n",
        "\n",
        "  def transform(self, X):\n",
        "    #X = X[self.column].replace(self.contraction)\n",
        "    return X[self.column].replace(self.contraction).fillna('').apply(lambda x: self.preprocessing(x)) "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-XAtbFVI24a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Processor:\n",
        "\n",
        "  def __init__(self, data, \n",
        "               contraction,\n",
        "               max_length = None):\n",
        "    \n",
        "    self.max_length = max_length\n",
        "    self.data = data\n",
        "    self.contraction = contraction\n",
        "    #self.textproc = textproc\n",
        "\n",
        "  def cleaning(self, data):\n",
        "    if not isinstance(data, pd.DataFrame):\n",
        "      raise TypeError('Only Dataframes are allowed, but got data={}'.format(data))\n",
        "    else:\n",
        "      print('Please wait, we are cleaning...')\n",
        "      titles = ['headline', 'text']\n",
        "      data_dict = {}\n",
        "      good = []\n",
        "\n",
        "      for title in titles:\n",
        "        proc = TextProccessing(title, self.contraction)\n",
        "        res = proc.fit_transform(data)\n",
        "        if title == 'headline':    \n",
        "          for tt in res:\n",
        "            row = ' '.join(tt.split('\\n'))\n",
        "            good.append(row.strip())\n",
        "        data_dict[title] = res\n",
        "\n",
        "      dd = pd.DataFrame(good)\n",
        "      dd.columns = ['headline']\n",
        "      ss = pd.DataFrame(data_dict)\n",
        "      ss = ss.drop(columns='headline', axis=1)\n",
        "      dataset = pd.concat([ss, dd], axis=1)\n",
        "      \n",
        "      return dataset\n",
        "\n",
        "  @staticmethod\n",
        "  def split(data):\n",
        "    return len(data.split())\n",
        "\n",
        "  def get_max_tokens(self, dataframe):\n",
        "\n",
        "    dataframe['article_len'] = dataframe['text'].apply(self.split)\n",
        "    dataframe['summary_len'] = dataframe['headline'].apply(self.split)\n",
        "    \n",
        "    max_len = {'max_article_tokens': max(dataframe['article_len']), \n",
        "                'min_article_tokens': min(dataframe['article_len']),\n",
        "                'max_summary_tokens': max(dataframe['summary_len']),\n",
        "                'min_summary_tokens': min(dataframe['summary_len'])}\n",
        "    return max_len, dataframe\n",
        "\n",
        "  def cleaned_data(self):\n",
        "    dataframe = self.cleaning(self.data)\n",
        "    \n",
        "    if self.max_length is None:\n",
        "      max_len, dataframe = self.get_max_tokens(dataframe)\n",
        "    else:\n",
        "      max_len = self.max_length\n",
        "\n",
        "    min_length = max_len['min_article_tokens']\n",
        "    idx_list = []\n",
        "    for idx, paragraph in tqdm(enumerate(dataframe)):\n",
        "        pragraph = len(paragraph.split())\n",
        "        if pragraph <= min_length:\n",
        "          idx_list.append(idx)\n",
        "    final_data = dataframe.drop(idx_list)\n",
        "    final_data = final_data.reset_index(drop=True)\n",
        "\n",
        "    _, final_data = self.get_max_tokens(final_data)\n",
        "\n",
        "\n",
        "    print('Done!')\n",
        "    return final_data, max_len"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNRiUhXoVkvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = {'max_article_tokens': 512,\n",
        "          'max_summary_tokens': 300,\n",
        "          'min_article_tokens': 30,\n",
        "          'min_summary_tokens': 0}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBiSvEkhz2ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "54ac741b33df4657a497c5a20990d260",
            "5662a03be19444fab76619ad47449786",
            "8e176b55e3d44509bf08563d814a22cc",
            "75a74d040dd0496d8dadbe91152a4364",
            "e293b28a9f364f75ace7642678abd828",
            "a6379aa1f5724141a274ff17c2e82f01",
            "6630ef87ba4b471caf23a0e368ceb791",
            "89f558dbc0694ff489eb6b77400cb5c3"
          ]
        },
        "outputId": "3bdd9041-10a5-48e1-f9c6-202b3fd2e35a"
      },
      "source": [
        "textproc = Processor(new_wikihowAll,contraction)#, maxlen)\n",
        "dataset, max_len = textproc.cleaned_data()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please wait, we are cleaning...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54ac741b33df4657a497c5a20990d260",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNmq9Mgx_BNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "23ca8cfd-d5a9-4ca7-faae-626c2de65a4a"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 214294 entries, 0 to 214293\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   text         214294 non-null  object\n",
            " 1   headline     214294 non-null  object\n",
            " 2   article_len  214294 non-null  int64 \n",
            " 3   summary_len  214294 non-null  int64 \n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 6.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngCA4TwV1YBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ad8e711b-50db-4c92-c79d-c53bac1de9b4"
      },
      "source": [
        "max_len"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_article_tokens': 12394,\n",
              " 'max_summary_tokens': 4341,\n",
              " 'min_article_tokens': 0,\n",
              " 'min_summary_tokens': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9chLqw-PK9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "e2cebd34-52fa-4767-d432-c8b0c9af6d9d"
      },
      "source": [
        "dataset['summary_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5000      44.0000\n",
              "0.7500      77.0000\n",
              "0.9000     122.0000\n",
              "0.9500     159.0000\n",
              "0.9900     266.0000\n",
              "0.9990     503.0000\n",
              "0.9999    1024.9796\n",
              "Name: summary_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBlW5wqJpmJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "7a8991bf-5f84-4e1a-9a00-e071c1ee35e3"
      },
      "source": [
        "dataset['article_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5000     306.0000\n",
              "0.7500     550.0000\n",
              "0.9000    1049.0000\n",
              "0.9500    1484.0000\n",
              "0.9900    2364.0700\n",
              "0.9990    3665.1210\n",
              "0.9999    5201.5503\n",
              "Name: article_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFFt1a_35GZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ef98887-4e25-4094-90c9-f43715a45121"
      },
      "source": [
        "art = int(dataset['article_len'].quantile(0.9900))\n",
        "sum = int(dataset['summary_len'].quantile(0.9900))\n",
        "\n",
        "print(f'art len:{art}, sum len:{sum}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "art len:2364, sum len:266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtuTFeLttppa",
        "colab_type": "text"
      },
      "source": [
        "#### Creating Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYEBtk-Ctyy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataReader(Dataset):\n",
        "  \"\"\"\n",
        "  Loading data to be used in pytorch pre-trained models\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, dataframe, T5tokenizer, **kwargs):\n",
        "    self.tokenizer = T5tokenizer\n",
        "    self.dataframe = dataframe\n",
        "    self.articles = dataframe.text\n",
        "    self.summary  = dataframe.headline\n",
        "    self.source_len = kwargs.get('max_article_tokens', 512)\n",
        "    self.target_len = kwargs.get('max_summary_tokens', 150)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.articles)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    articles = self.articles[idx]\n",
        "    articles = ' '.join(articles.split())\n",
        "\n",
        "    summary = self.summary[idx]\n",
        "    summary = ' '.join(summary.split())\n",
        "\n",
        "    source_tokenized = self.tokenizer.batch_encode_plus([articles], max_length= self.source_len, pad_to_max_length=True, truncation=True,return_tensors='pt')\n",
        "    target_tokenized = self.tokenizer.batch_encode_plus([summary], max_length= self.target_len, pad_to_max_length=True, truncation=True,return_tensors='pt')\n",
        "\n",
        "    source_ids = source_tokenized['input_ids'].squeeze(0)\n",
        "    source_mask = source_tokenized['attention_mask'].squeeze(0)\n",
        "\n",
        "    summary_ids = target_tokenized['input_ids'].squeeze(0)\n",
        "    summary_mask = target_tokenized['attention_mask'].squeeze(0)\n",
        "\n",
        "    return source_ids, source_mask, summary_ids, summary_mask\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqfiCrePYQIK",
        "colab_type": "text"
      },
      "source": [
        "######Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQwJS66iz0uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model, dataset_loader, optimizer, scheduler, iteration=0):\n",
        "\n",
        "  model.train()\n",
        "  avg_loss = 0\n",
        "\n",
        "\n",
        "  print(f'Start training for iteration: {iteration}')\n",
        " \n",
        "  for idx, data_val in enumerate(dataset_loader):\n",
        "\n",
        "    input_ids, source_mask, summary_ids, summary_mask = data_val\n",
        "\n",
        "    input_ids = input_ids.to(device, dtype = torch.long)\n",
        "    source_mask = source_mask.to(device, dtype = torch.float)\n",
        "    summary_ids = summary_ids.to(device, dtype = torch.long)\n",
        "\n",
        "    target_labels = summary_ids[:, :-1].contiguous()\n",
        "    langm_labels = summary_ids[:, 1:].clone().detach()\n",
        "    langm_labels[summary_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(input_ids=input_ids, attention_mask = source_mask, decoder_input_ids=target_labels, lm_labels=langm_labels, output_hidden_states=True)\n",
        "    \n",
        "    loss, prediction_scores, hidden_states = output[:3]\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print(f'The loss is:{loss:.3f} and of: {idx+1}/{len(train_set)} batches')\n",
        "  \n",
        "  return loss, prediction_scores, hidden_states\n",
        "\n",
        "\n",
        "  \n",
        "  #bar.finish()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51Hk0xEz0sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train, Validation and Test splits { run: \"auto\" }\n",
        "dataset.text = \"summarize: \" + dataset.text # for T5 conformity\n",
        "\n",
        "columns = [\"text\",\"headline\"]\n",
        "df = dataset[columns]\n",
        "X_train, X_test = train_test_split(df, test_size=0.1, random_state = 42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQeVMTtGz0p7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2f7255da-5ab1-4e23-b604-6d46b231de79"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>summarize: this calendar of events is availabl...</td>\n",
              "      <td>consult the formula 1 race calendar., pick an ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>summarize: once you ve finished your self anal...</td>\n",
              "      <td>fix the problem., make a conscious effort to l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>summarize: stir it thoroughly and then set it ...</td>\n",
              "      <td>add 1 teaspoon 4.9Â ml of borax to 500 millilit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>summarize: recognizing that you need to establ...</td>\n",
              "      <td>decide to set boundaries., define the boundary...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>summarize: communicate with your significant o...</td>\n",
              "      <td>talk with your significant other about your re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                           headline\n",
              "0  summarize: this calendar of events is availabl...  consult the formula 1 race calendar., pick an ...\n",
              "1  summarize: once you ve finished your self anal...  fix the problem., make a conscious effort to l...\n",
              "2  summarize: stir it thoroughly and then set it ...  add 1 teaspoon 4.9Â ml of borax to 500 millilit...\n",
              "3  summarize: recognizing that you need to establ...  decide to set boundaries., define the boundary...\n",
              "4  summarize: communicate with your significant o...  talk with your significant other about your re..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_SPfP6z0nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_size = 0.8\n",
        "# train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
        "# val_dataset=df.drop(train_dataset.index).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582OWlZz0kd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "0901cd5f-0993-4a5a-9e8f-96c7932f38fd"
      },
      "source": [
        "d = ' '.join(X_train.iloc[0]['text'].split())\n",
        "d"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'summarize: this calendar of events is available on the official internet site of formula 1. the official formula 1 internet site also lists the scheduled times for the friday free practice sessions, the saturday free practice and race qualifying sessions, and the sunday race. there is some variation in race venues from year to year, so be sure to use the current race calendar. , the early season races will be more open to all competitors while in the later races, the competition will increasingly focus on those who still have a chance to be world champion. , the official formula 1 internet calendar will offer track maps for all events. these are not standard ovals, but rather complex courses with turns of varying difficulty and straights of varying lengths. choose a slower, twisty track or a wide, high speed track with long straights. , be aware that the typical weather expectations for each race venue will differ. for example, it may be hot at the turkish grand prix and rainy at the belgian grand prix. , the need for passports and visas will vary from country to country, and advice from the appropriate embassy or national travel office should be obtained. determine any other restrictions that may apply. for example, alcohol is illegal in muslim countries.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy11wmiBz0iE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e7267e4f-b7dd-4540-ef10-4030b062a4e0"
      },
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "MODEL_NAME = 't5-base' # or T5-Large:770 params, T5-3B, T5-Base: 220 Params, T5-11B\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08lW8ejhmgya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(model)\n",
        "X_train = X_train.iloc[:1000]\n",
        "X_test = X_test.iloc[:300]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDGi1RVicmTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {'max_article_tokens': 512,\n",
        "          'max_summary_tokens': 150}\n",
        "\n",
        "train_set = CustomDataReader(X_train, tokenizer,**args)\n",
        "val_set = CustomDataReader(X_test, tokenizer,**args)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05TKZmNgkr8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "train_params = {\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 2\n",
        "        }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 2\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8K6fAvaz0fg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "2a7820bd-2aa2-4041-ae0e-650c15c9bd64"
      },
      "source": [
        "N_EPOCHS = 2\n",
        "best_loss = float('inf')\n",
        "train_loss = []\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "  loss, prediction_scores, hidden_states = training(model, train_loader, optimizer, i)\n",
        "  train_loss.append(loss.item())\n",
        "\n",
        "  if loss < best_loss:\n",
        "    best_loss = loss\n",
        "    torch.save(model.state_dict(), 'best_model.pth')\n",
        "print(f'Best loss is: {best_loss}')\n",
        "  #scheduler.step(prediction_scores)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training for iteration: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-4ea25aebefb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9243f124be3e>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, dataset_loader, optimizer, scheduler, iteration)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlangm_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, decoder_past_key_value_states, use_cache, labels, inputs_embeds, decoder_inputs_embeds, head_mask, output_attentions, output_hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mpast_key_value_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m             )\n\u001b[1;32m    750\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value_state, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mnorm_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.43 GiB total capacity; 6.48 GiB already allocated; 20.94 MiB free; 6.81 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsxZGr_bz0cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(model, dataset_loader, iteration):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for idx, data_val in enumerate(dataset_loader):\n",
        "\n",
        "    input_ids, source_mask, summary_ids, summary_mask = data_val\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KX71gPzy_bH",
        "colab_type": "text"
      },
      "source": [
        "######Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ter9ktdQ6_54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric(labels, gen_target):\n",
        "  print(f'Start evaluation')\n",
        "  bar = progressbar.ProgressBar(maxval=len(dataset_loader))\n",
        "  bar.start()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  bar.finish()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTkqfrwoy5QR",
        "colab_type": "text"
      },
      "source": [
        "##### Freezing layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDjdxQUI6_1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.decoder.block[11].requires_grad=True\n",
        "model.decoder.block[4].requires_grad=True\n",
        "\n",
        "for param model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfdlo_5_6_xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqWc1IdF6_tA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7FyuTjN6_om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O8oTENj6_kI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKLE0jCV6_Wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IElLmIW36_Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Be5LE3q6_O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imlmpMSC6_K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CLDteX96_Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOSEmtEe6_Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCbkYzyj6-7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x15lYdFGz0XL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NrQfhPSz0UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYnfvPt3z0Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OljFlfM5z0OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class T5Model(nn.Module):\n",
        "\n",
        "  def __init__(self, t5, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkpccxfe_Rwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DictionaryDataset:\n",
        "  def __init__(self, dataset=None):\n",
        "    self.w2index = defaultdict(int)\n",
        "    self.index2w = defaultdict(lambda: 'SOS')\n",
        "    self.symbols = ['SOS','EOS','PAD']\n",
        "    self.wcount = 3\n",
        "\n",
        "    for i, s in enumerate(self.symbols):\n",
        "      self.index2w[i] = s\n",
        "    for i, s in enumerate(self.symbols):\n",
        "      self.w2index[s] = i\n",
        "      \n",
        "    if dataset:\n",
        "      for i, line in tqdm(enumerate(dataset)):\n",
        "        self.word2index(line)\n",
        "\n",
        "  def word2index(self, sentence):\n",
        "    words = sentence.split()\n",
        "    for word in words:\n",
        "      if word not in self.w2index.keys():\n",
        "        self.w2index[word] = self.wcount\n",
        "        self.index2w[self.wcount] = word\n",
        "        self.wcount +=1\n",
        "    \n",
        "  def encode_sequence(self, dataset):\n",
        "    for i, line in tqdm(enumerate(dataset)):\n",
        "      self.word2index(line)\n",
        "\n",
        "  def decode_idx(self, idx):\n",
        "    dec_words = []\n",
        "    for i in idx:\n",
        "      dec_words.append(self.index2w[i])\n",
        "    return dec_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01oJ3XRCSBV",
        "colab_type": "text"
      },
      "source": [
        "![LSTM](https://drive.google.com/open?id=19pYBGaKGx2wTHjDC3QLMkpr5HDqTPbb6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujAMkc2nDPaB",
        "colab_type": "text"
      },
      "source": [
        "![Seq2SeqWithAttention](https://drive.google.com/open?id=1QDLNN0XQ0TfeA-nyAGBhG3YgwAX0bPQU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfvg8MT3RBFI",
        "colab_type": "text"
      },
      "source": [
        "You should use the same number of layers, for encoder and decoder\n",
        "shape for inputs is (batch size, sequence length), (batch size, 1) respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF-TwabDk4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size, embed_size, num_hidden, batch_size, num_layers=3, dropout = 0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_layers = num_layers\n",
        "    self.input_size = input_size\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.embed = nn.Embedding(input_size, embed_size)\n",
        "    self.lstm = nn.LSTM(input_size=embed_size, hidden_size=num_hidden,num_layers = num_layers, dropout=(0 if num_layers ==2 else dropout))\n",
        "\n",
        "  def init_hidden(self):\n",
        "     # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "    return (torch.zeros(self.num_layers, self.batch_size, self.num_hidden), torch.zeros(self.num_layers, self.batch_size, self.num_hidden))\n",
        "  def forward(self, input, hidden):\n",
        "    # input (batch size, sequence length)\n",
        "    embedding = self.embed(input)\n",
        "    #print(embedding.shape)\n",
        "    embed_out = embedding.transpose(0,1)\n",
        "    print(embed_out.shape)\n",
        "    lstm_out, hidden = self.lstm(embed_out, hidden)\n",
        "\n",
        "    # The shape of lstm_out is (seq_len, batch_size, num_hiddens).\n",
        "    # state contains the hidden state and the memory cell\n",
        "    # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n",
        "    #lstout is (seq_length, batch_size, num_hiddens)\n",
        "    return lstm_out, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4_h0bXp0cl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1310d376-e495-41c8-fd80-730b4970dca2"
      },
      "source": [
        "encoder = Encoder(input_size =10, embed_size=8, num_hidden=16, batch_size=4)\n",
        "X = torch.zeros((4, 7),dtype=torch.long)\n",
        "hidden = encoder.init_hidden()\n",
        "output, hidden = encoder(X, hidden)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7, 4, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ53T62V6ZOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70b9e373-d6ba-4eed-da7d-b002257ab62b"
      },
      "source": [
        "hidden[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XNO21wuBTGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab86113b-05af-4271-d479-df115a533fd9"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7, 4, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e55gItUrF_wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size, method='dot'):\n",
        "    super(Attention, self).__init__()\n",
        "    self.method = method\n",
        "    self.hidden_size = hidden_size\n",
        "  def dot_score(self, hidden, encoder_output):\n",
        "    return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "  def forward(self, hidden, encoder_output):\n",
        "    att_prod = self.dot_score(hidden, encoder_output)\n",
        "    attn_weights = att_prod.t()\n",
        "    return F.softmax(attn_weights, dim=1).unsqueeze(1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp5VhnhRnuaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderWithAtt(nn.Module):\n",
        "  def __init__(self, embed_size_in, num_hidden, output_size, att_method=None, num_layers = 3):\n",
        "    super(DecoderWithAtt, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.num_hidden = num_hidden\n",
        "    self.output_size = output_size\n",
        "    self.att_method = att_method\n",
        "\n",
        "    self.emb = nn.Embedding(embed_size_in, num_hidden)\n",
        "    self.lstm = nn.LSTM(input_size=num_hidden, hidden_size=num_hidden, num_layers = num_layers)\n",
        "    self.con_ln = nn.Linear(num_hidden * 2, num_hidden)\n",
        "    self.drop = nn.Dropout(p=0.2)\n",
        "    self.fc  = nn.Linear(self.num_hidden, self.output_size)\n",
        "    self.attention = Attention(att_method, num_hidden)\n",
        "\n",
        "  def forward(self, prev_hidden, target_input, enconder_outputs):\n",
        "    embed_out = self.emb(target_input).transpose(0,1)\n",
        "    embed_out = self.drop(embed_out)\n",
        "    print('embed out >>>', embed_out.shape)\n",
        "    output, hidden = self.lstm(embed_out, prev_hidden) # shape (1, batch_size, hidden_size), 1 for one time step word \n",
        "    print('lstm out >>>', output.shape)\n",
        "    print('hidden out >>>', hidden[0].shape)\n",
        "\n",
        "    attent_weights = self.attention(output, enconder_outputs) # (att out shape: batch_size, 1, max_length), enc in shape: (max_length, batch_size, hidden_size)\n",
        "    print('attent_weights out >>>', attent_weights.shape)\n",
        "    print('enconder_outputs >>>', enconder_outputs.shape)\n",
        "\n",
        "    context = torch.bmm(attent_weights, enconder_outputs.transpose(0,1))\n",
        "    print('context out >>>', context.shape)\n",
        "    cont_dec_out = torch.cat((context, output.transpose(0,1)), 1)\n",
        "    print('to linear out >>>', cont_dec_out.shape)\n",
        "    cont_dec_out = cont_dec_out.view(cont_dec_out.shape[0], -1)\n",
        "    concat_out = self.con_ln(cont_dec_out)\n",
        "    concat_out = torch.tanh(concat_out)\n",
        "\n",
        "    out = self.fc(concat_out)\n",
        "    out = F.softmax(out, dim=1)\n",
        "\n",
        "    return out, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9oa_EJ6_GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = DecoderWithAtt(10,16,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHklHQXG7x4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "eeeb0cd9-0cd2-424f-dc6a-3823273e7fdb"
      },
      "source": [
        "x = torch.zeros((4, 1),dtype=torch.long)\n",
        "out, hidden = decoder(hidden, x, output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed out >>> torch.Size([1, 4, 16])\n",
            "lstm out >>> torch.Size([1, 4, 16])\n",
            "hidden out >>> torch.Size([3, 4, 16])\n",
            "attent_weights out >>> torch.Size([4, 1, 7])\n",
            "enconder_outputs >>> torch.Size([7, 4, 16])\n",
            "context out >>> torch.Size([4, 1, 16])\n",
            "to linear out >>> torch.Size([4, 2, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW94xBGOOPQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cc0b4a1-9b8f-4a40-db3a-0a809f844f35"
      },
      "source": [
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSB0L7_6QyEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b4d1c40-e5b1-446d-c843-4deb13218d54"
      },
      "source": [
        "hidden[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rl0M8_pQ6sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}