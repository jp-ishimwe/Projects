{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jp-ishimwe/Projects/blob/master/T5_ftransfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxxkIi-xDrUz"
   },
   "outputs": [],
   "source": [
    "#@title Import packages { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import progressbar\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "!pip install transformers -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "LME26NCmM8ne",
    "outputId": "f4d7fc18-3fce-4068-97ea-bc9f6f88f42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "7rF67H7GnK2U",
    "outputId": "80c73f68-6939-4b99-f996-2f73850b6c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Driver/library version mismatch\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!/usr/local/cuda/bin/nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1g1xKdoCnMAq",
    "outputId": "e58f21b3-7fcd-4d6d-c81a-8a75f5ddc606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#@title To proceed should be cuda { run: \"auto\", vertical-output: true }\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jvIoL5I0Eb5"
   },
   "source": [
    "#### Getting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGVOHhrk0GOP"
   },
   "source": [
    "You can find the dataset [here](https://github.com/mahnazkoupaee/WikiHow-Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkDYkbKV0SUM"
   },
   "source": [
    "**Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5sAQxsN4M8k-",
    "outputId": "a2697df8-3085-47f8-88e7-c40e7368c473"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/My Drive/Colab Notebooks/wikihowAll.csv') as f:\n",
    "#   wikihowAll = pd.read_csv(f)\n",
    "# print(f'The size of data:{wikihowAll.shape}')\n",
    "\n",
    "wikihowAll = pd.read_csv('wikihowAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "eia69USAvek8",
    "outputId": "9de5cb09-297c-440d-b60e-22b33590f504"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nKeep related supplies in the same area.,\\nMa...</td>\n",
       "      <td>How to Be an Organized Artist1</td>\n",
       "      <td>If you're a photographer, keep all the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nCreate a sketch in the NeoPopRealist manner ...</td>\n",
       "      <td>How to Create a Neopoprealist Art Work</td>\n",
       "      <td>See the image for how this drawing develops s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  ...                                               text\n",
       "0  \\nKeep related supplies in the same area.,\\nMa...  ...   If you're a photographer, keep all the necess...\n",
       "1  \\nCreate a sketch in the NeoPopRealist manner ...  ...   See the image for how this drawing develops s...\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihowAll.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JX6clAc9wHuy"
   },
   "outputs": [],
   "source": [
    "contraction = {\"isn't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you ll\":\"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you re\": \"you are\" , \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E38ktgfStkI9"
   },
   "source": [
    "###### Checking for NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "b-NTfClirfL3",
    "outputId": "e95ccecc-c584-4111-f5c2-506840dbe7d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline     818\n",
       "title          1\n",
       "text        1071\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihowAll.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jF6jYdXItg7W"
   },
   "source": [
    "we have 818 articles without summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOAJwnKks87U"
   },
   "outputs": [],
   "source": [
    "new_wikihowAll = wikihowAll[['text', 'headline']]\n",
    "new_wikihowAll = new_wikihowAll.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "KGJZY7L2tTWG",
    "outputId": "7f682db0-d585-4425-cbde-cbe8af68f52e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        253\n",
       "headline      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wikihowAll.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6BJG-DsuQ4F"
   },
   "outputs": [],
   "source": [
    "indices = np.where(new_wikihowAll['text'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "GJ2ZXsLmuuIY",
    "outputId": "fd37ad08-4b50-4d8a-819b-f89c6550b892"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Colour coordinate your looks (For all Events)\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nRecognize the following symptoms:\\n\\nBloatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nAt each practice, teach all of your squad di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7228</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Understand that these directions assume the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dribble with your left or right hand, whicheve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nStretch each arm as far across your chest wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210787</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nDownshift to a lower gear to get more power ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe next thing you need to do is show Invisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213977</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nIt is the same as the chorus, but with the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214419</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nLower the fifth 1/2 step (make the D into a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                           headline\n",
       "1492    NaN  Colour coordinate your looks (For all Events)\\n\\n\n",
       "4328    NaN  \\nRecognize the following symptoms:\\n\\nBloatin...\n",
       "7190    NaN  \\nAt each practice, teach all of your squad di...\n",
       "7228    NaN  Understand that these directions assume the re...\n",
       "7378    NaN  Dribble with your left or right hand, whicheve...\n",
       "...     ...                                                ...\n",
       "208605  NaN  \\nStretch each arm as far across your chest wh...\n",
       "210787  NaN  \\nDownshift to a lower gear to get more power ...\n",
       "212574  NaN  \\nThe next thing you need to do is show Invisi...\n",
       "213977  NaN  \\nIt is the same as the chorus, but with the s...\n",
       "214419  NaN  \\nLower the fifth 1/2 step (make the D into a ...\n",
       "\n",
       "[253 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wikihowAll.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lNQP3dX3vzaD",
    "outputId": "7f5dd79d-7b50-48e3-b7b5-5db0c2c062e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        0\n",
       "headline    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wikihowAll = new_wikihowAll.dropna().reset_index(drop=True)\n",
    "new_wikihowAll.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fbFoCHsVv9Fw",
    "outputId": "5b94bf08-4ab8-4c40-925c-3170fb5920dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214294, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wikihowAll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wBbyQwv5P1A"
   },
   "outputs": [],
   "source": [
    "# h = new_wikihowAll\n",
    "\n",
    "# for x in ['text', 'headline']:\n",
    "# \t\th[x+'_tokenized'] = h[x].apply(lambda x:x.lower().split() );\n",
    "# \t\th[x+'_len'] = h[x+'_tokenized'].apply(lambda x: len(x))\n",
    "# min(h['headline_len']), min(h['text_len']), len(h[h['headline_len']==1]), len(h[h['text_len']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0HQ2wJrXEAcr"
   },
   "source": [
    "#####Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_qmLtaZp9s0"
   },
   "outputs": [],
   "source": [
    "class TextProccessing(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, column, contraction):\n",
    "    self.column = column\n",
    "    self.contraction = contraction\n",
    "  def fit(self, x, y=None):\n",
    "    return self\n",
    "  @staticmethod\n",
    "  def preprocessing(line):\n",
    "    \n",
    "    line = line.lower() \n",
    "    line = re.sub(r\"http\\S+\", \" \",line)\n",
    "    line = re.sub(r'[^\\sa-zA-Z0-9.,!?]',' ',line)\n",
    "    line = line.replace('-', ' ')\n",
    "    line = line.strip(' ')\n",
    "    line = re.sub(r'\\s{2,}',' ', line)\n",
    "\n",
    "    return line\n",
    "\n",
    "  def transform(self, X):\n",
    "    #X = X[self.column].replace(self.contraction)\n",
    "    return X[self.column].replace(self.contraction).fillna('').apply(lambda x: self.preprocessing(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-XAtbFVI24a"
   },
   "outputs": [],
   "source": [
    "class Processor:\n",
    "\n",
    "  def __init__(self, data, \n",
    "               contraction,\n",
    "               max_length = None):\n",
    "    \n",
    "    self.max_length = max_length\n",
    "    self.data = data\n",
    "    self.contraction = contraction\n",
    "    #self.textproc = textproc\n",
    "\n",
    "  def cleaning(self, data):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "      raise TypeError('Only Dataframes are allowed, but got data={}'.format(data))\n",
    "    else:\n",
    "      print('Please wait, we are cleaning...')\n",
    "      titles = ['headline', 'text']\n",
    "      data_dict = {}\n",
    "      good = []\n",
    "\n",
    "      for title in titles:\n",
    "        proc = TextProccessing(title, self.contraction)\n",
    "        res = proc.fit_transform(data)\n",
    "        if title == 'headline':    \n",
    "          for tt in res:\n",
    "            row = ' '.join(tt.split('\\n'))\n",
    "            good.append(row.strip())\n",
    "        data_dict[title] = res\n",
    "\n",
    "      dd = pd.DataFrame(good)\n",
    "      dd.columns = ['headline']\n",
    "      ss = pd.DataFrame(data_dict)\n",
    "      ss = ss.drop(columns='headline', axis=1)\n",
    "      dataset = pd.concat([ss, dd], axis=1)\n",
    "      \n",
    "      return dataset\n",
    "\n",
    "  @staticmethod\n",
    "  def split(data):\n",
    "    return len(data.split())\n",
    "\n",
    "  def get_max_tokens(self, dataframe):\n",
    "\n",
    "    dataframe['article_len'] = dataframe['text'].apply(self.split)\n",
    "    dataframe['summary_len'] = dataframe['headline'].apply(self.split)\n",
    "    \n",
    "    max_len = {'max_article_tokens': max(dataframe['article_len']), \n",
    "                'min_article_tokens': min(dataframe['article_len']),\n",
    "                'max_summary_tokens': max(dataframe['summary_len']),\n",
    "                'min_summary_tokens': min(dataframe['summary_len'])}\n",
    "    return max_len, dataframe\n",
    "\n",
    "  def cleaned_data(self):\n",
    "    dataframe = self.cleaning(self.data)\n",
    "    \n",
    "    if self.max_length is None:\n",
    "      max_len, dataframe = self.get_max_tokens(dataframe)\n",
    "    else:\n",
    "      max_len = self.max_length\n",
    "\n",
    "    min_length = max_len['min_article_tokens']\n",
    "    idx_list = []\n",
    "    for idx, paragraph in tqdm(enumerate(dataframe)):\n",
    "        pragraph = len(paragraph.split())\n",
    "        if pragraph <= min_length:\n",
    "          idx_list.append(idx)\n",
    "    final_data = dataframe.drop(idx_list)\n",
    "    final_data = final_data.reset_index(drop=True)\n",
    "\n",
    "    _, final_data = self.get_max_tokens(final_data)\n",
    "\n",
    "\n",
    "    print('Done!')\n",
    "    return final_data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNRiUhXoVkvY"
   },
   "outputs": [],
   "source": [
    "maxlen = {'max_article_tokens': 512,\n",
    "          'max_summary_tokens': 300,\n",
    "          'min_article_tokens': 30,\n",
    "          'min_summary_tokens': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "54ac741b33df4657a497c5a20990d260",
      "5662a03be19444fab76619ad47449786",
      "8e176b55e3d44509bf08563d814a22cc",
      "75a74d040dd0496d8dadbe91152a4364",
      "e293b28a9f364f75ace7642678abd828",
      "a6379aa1f5724141a274ff17c2e82f01",
      "6630ef87ba4b471caf23a0e368ceb791",
      "89f558dbc0694ff489eb6b77400cb5c3"
     ]
    },
    "colab_type": "code",
    "id": "oBiSvEkhz2ol",
    "outputId": "3bdd9041-10a5-48e1-f9c6-202b3fd2e35a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, we are cleaning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc173f5987db46e9af6a6cfd48d7e821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "textproc = Processor(new_wikihowAll,contraction)#, maxlen)\n",
    "dataset, max_len = textproc.cleaned_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "MNmq9Mgx_BNx",
    "outputId": "23ca8cfd-d5a9-4ca7-faae-626c2de65a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214294 entries, 0 to 214293\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   text         214294 non-null  object\n",
      " 1   headline     214294 non-null  object\n",
      " 2   article_len  214294 non-null  int64 \n",
      " 3   summary_len  214294 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ngCA4TwV1YBW",
    "outputId": "ad8e711b-50db-4c92-c79d-c53bac1de9b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_article_tokens': 12394,\n",
       " 'max_summary_tokens': 4341,\n",
       " 'min_article_tokens': 0,\n",
       " 'min_summary_tokens': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "x9chLqw-PK9e",
    "outputId": "e2cebd34-52fa-4767-d432-c8b0c9af6d9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000      44.0000\n",
       "0.7500      77.0000\n",
       "0.9000     122.0000\n",
       "0.9500     159.0000\n",
       "0.9900     266.0000\n",
       "0.9990     503.0000\n",
       "0.9999    1024.9796\n",
       "Name: summary_len, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['summary_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "hBlW5wqJpmJK",
    "outputId": "7a8991bf-5f84-4e1a-9a00-e071c1ee35e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000     306.0000\n",
       "0.7500     550.0000\n",
       "0.9000    1049.0000\n",
       "0.9500    1484.0000\n",
       "0.9900    2364.0700\n",
       "0.9990    3665.1210\n",
       "0.9999    5201.5503\n",
       "Name: article_len, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['article_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bFFt1a_35GZR",
    "outputId": "2ef98887-4e25-4094-90c9-f43715a45121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art len:2364, sum len:266\n"
     ]
    }
   ],
   "source": [
    "art = int(dataset['article_len'].quantile(0.9900))\n",
    "sum = int(dataset['summary_len'].quantile(0.9900))\n",
    "\n",
    "print(f'art len:{art}, sum len:{sum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtuTFeLttppa"
   },
   "source": [
    "#### Creating Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYEBtk-Ctyy4"
   },
   "outputs": [],
   "source": [
    "class CustomDataReader(Dataset):\n",
    "  \"\"\"\n",
    "  Loading data to be used in pytorch pre-trained models\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, dataframe, T5tokenizer, **kwargs):\n",
    "    self.tokenizer = T5tokenizer\n",
    "    self.dataframe = dataframe\n",
    "    self.articles = dataframe.text\n",
    "    self.summary  = dataframe.headline\n",
    "    self.source_len = kwargs.get('max_article_tokens', 512)\n",
    "    self.target_len = kwargs.get('max_summary_tokens', 150)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.articles)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    articles = self.articles[idx]\n",
    "    articles = ' '.join(articles.split())\n",
    "\n",
    "    summary = self.summary[idx]\n",
    "    summary = ' '.join(summary.split())\n",
    "\n",
    "    source_tokenized = self.tokenizer.batch_encode_plus([articles], max_length= self.source_len, pad_to_max_length=True, truncation=True,return_tensors='pt')\n",
    "    target_tokenized = self.tokenizer.batch_encode_plus([summary], max_length= self.target_len, pad_to_max_length=True, truncation=True,return_tensors='pt')\n",
    "\n",
    "    source_ids = source_tokenized['input_ids'].squeeze(0)\n",
    "    source_mask = source_tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "    summary_ids = target_tokenized['input_ids'].squeeze(0)\n",
    "    summary_mask = target_tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "    return source_ids, source_mask, summary_ids, summary_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqfiCrePYQIK"
   },
   "source": [
    "######Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQwJS66iz0uS"
   },
   "outputs": [],
   "source": [
    "def training(model, dataset_loader, optimizer, iteration=0):\n",
    "\n",
    "  model.train()\n",
    "  avg_loss = 0\n",
    "\n",
    "\n",
    "  print(f'Start training for iteration: {iteration}')\n",
    " \n",
    "  for idx, data_val in enumerate(dataset_loader):\n",
    "\n",
    "    input_ids, source_mask, summary_ids, summary_mask = data_val\n",
    "\n",
    "    input_ids = input_ids.to(device, dtype = torch.long)\n",
    "    source_mask = source_mask.to(device, dtype = torch.float)\n",
    "    summary_ids = summary_ids.to(device, dtype = torch.long)\n",
    "\n",
    "    target_labels = summary_ids[:, :-1].contiguous()\n",
    "    langm_labels = summary_ids[:, 1:].clone().detach()\n",
    "    langm_labels[summary_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(input_ids=input_ids, attention_mask = source_mask, decoder_input_ids=target_labels, lm_labels=langm_labels, output_hidden_states=True)\n",
    "    \n",
    "    loss, prediction_scores, hidden_states = output[:3]\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if idx % 100 == 0:\n",
    "      print(f'The loss is:{loss:.3f} and of: {idx+1}/{(len(train_set)/int(len(input_ids)))} batches')\n",
    "  \n",
    "  return loss, prediction_scores, hidden_states\n",
    "\n",
    "\n",
    "  \n",
    "  #bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W51Hk0xEz0sH"
   },
   "outputs": [],
   "source": [
    "#@title Train, Validation and Test splits { run: \"auto\" }\n",
    "dataset.text = \"summarize: \" + dataset.text # for T5 conformity\n",
    "\n",
    "columns = [\"text\",\"headline\"]\n",
    "df = dataset[columns]\n",
    "X_train, X_test = train_test_split(df, test_size=0.1, random_state = 42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "sQeVMTtGz0p7",
    "outputId": "2f7255da-5ab1-4e23-b604-6d46b231de79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: this calendar of events is availabl...</td>\n",
       "      <td>consult the formula 1 race calendar., pick an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: once you ve finished your self anal...</td>\n",
       "      <td>fix the problem., make a conscious effort to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize: stir it thoroughly and then set it ...</td>\n",
       "      <td>add 1 teaspoon 4.9 ml of borax to 500 millilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarize: recognizing that you need to establ...</td>\n",
       "      <td>decide to set boundaries., define the boundary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarize: communicate with your significant o...</td>\n",
       "      <td>talk with your significant other about your re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                                           headline\n",
       "0  summarize: this calendar of events is availabl...  consult the formula 1 race calendar., pick an ...\n",
       "1  summarize: once you ve finished your self anal...  fix the problem., make a conscious effort to l...\n",
       "2  summarize: stir it thoroughly and then set it ...  add 1 teaspoon 4.9 ml of borax to 500 millilit...\n",
       "3  summarize: recognizing that you need to establ...  decide to set boundaries., define the boundary...\n",
       "4  summarize: communicate with your significant o...  talk with your significant other about your re..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dm_SPfP6z0nH"
   },
   "outputs": [],
   "source": [
    "# train_size = 0.8\n",
    "# train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
    "# val_dataset=df.drop(train_dataset.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "G582OWlZz0kd",
    "outputId": "0901cd5f-0993-4a5a-9e8f-96c7932f38fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: this calendar of events is available on the official internet site of formula 1. the official formula 1 internet site also lists the scheduled times for the friday free practice sessions, the saturday free practice and race qualifying sessions, and the sunday race. there is some variation in race venues from year to year, so be sure to use the current race calendar. , the early season races will be more open to all competitors while in the later races, the competition will increasingly focus on those who still have a chance to be world champion. , the official formula 1 internet calendar will offer track maps for all events. these are not standard ovals, but rather complex courses with turns of varying difficulty and straights of varying lengths. choose a slower, twisty track or a wide, high speed track with long straights. , be aware that the typical weather expectations for each race venue will differ. for example, it may be hot at the turkish grand prix and rainy at the belgian grand prix. , the need for passports and visas will vary from country to country, and advice from the appropriate embassy or national travel office should be obtained. determine any other restrictions that may apply. for example, alcohol is illegal in muslim countries.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ' '.join(X_train.iloc[0]['text'].split())\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "Iy11wmiBz0iE",
    "outputId": "e7267e4f-b7dd-4540-ef10-4030b062a4e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-5\n",
    "MODEL_NAME = 't5-base' # or T5-Large:770 params, T5-3B, T5-Base: 220 Params, T5-11B\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08lW8ejhmgya"
   },
   "outputs": [],
   "source": [
    "#print(model)\n",
    "X_train = X_train.iloc[:2000]\n",
    "X_test = X_test.iloc[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDGi1RVicmTp"
   },
   "outputs": [],
   "source": [
    "args = {'max_article_tokens': 512,\n",
    "          'max_summary_tokens': 150}\n",
    "\n",
    "train_set = CustomDataReader(X_train, tokenizer,**args)\n",
    "val_set = CustomDataReader(X_test, tokenizer,**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05TKZmNgkr8V"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "train_params = {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2\n",
    "        }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 2\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "y8K6fAvaz0fg",
    "outputId": "2a7820bd-2aa2-4041-ae0e-650c15c9bd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for iteration: 0\n",
      "The loss is:9.027 and of: 1/500.0 batches\n",
      "The loss is:4.217 and of: 101/500.0 batches\n",
      "The loss is:3.788 and of: 201/500.0 batches\n",
      "The loss is:3.575 and of: 301/500.0 batches\n",
      "The loss is:3.113 and of: 401/500.0 batches\n",
      "Start training for iteration: 1\n",
      "The loss is:2.858 and of: 1/500.0 batches\n",
      "The loss is:2.771 and of: 101/500.0 batches\n",
      "The loss is:3.253 and of: 201/500.0 batches\n",
      "The loss is:2.725 and of: 301/500.0 batches\n",
      "The loss is:2.381 and of: 401/500.0 batches\n",
      "Start training for iteration: 2\n",
      "The loss is:3.025 and of: 1/500.0 batches\n",
      "The loss is:2.551 and of: 101/500.0 batches\n",
      "The loss is:2.729 and of: 201/500.0 batches\n",
      "The loss is:2.783 and of: 301/500.0 batches\n",
      "The loss is:2.878 and of: 401/500.0 batches\n",
      "Start training for iteration: 3\n",
      "The loss is:2.673 and of: 1/500.0 batches\n",
      "The loss is:2.233 and of: 101/500.0 batches\n",
      "The loss is:3.173 and of: 201/500.0 batches\n",
      "The loss is:2.306 and of: 301/500.0 batches\n",
      "The loss is:2.668 and of: 401/500.0 batches\n",
      "Start training for iteration: 4\n",
      "The loss is:2.714 and of: 1/500.0 batches\n",
      "The loss is:2.558 and of: 101/500.0 batches\n",
      "The loss is:2.611 and of: 201/500.0 batches\n",
      "The loss is:2.503 and of: 301/500.0 batches\n",
      "The loss is:1.860 and of: 401/500.0 batches\n",
      "Best loss is: 2.155\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "best_loss = float('inf')\n",
    "train_loss = []\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "  loss, prediction_scores, hidden_states = training(model, train_loader, optimizer, iteration=i)\n",
    "  train_loss.append(loss.item())\n",
    "\n",
    "  if best_loss > loss:\n",
    "    best_loss = loss\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "print(f'Best loss is: {best_loss:.3f}')\n",
    "  #scheduler.step(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsxZGr_bz0cv"
   },
   "outputs": [],
   "source": [
    "def validation(model, dataset_loader, iteration=0):\n",
    "  \n",
    "  model.eval()\n",
    "\n",
    "  results = {\"summary\": [], \"article\": []}\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for idx, data_val in enumerate(dataset_loader):\n",
    "      \n",
    "      input_ids, source_mask, summary_ids, summary_mask = data_val \n",
    "      \n",
    "      input_ids = input_ids.to(device, dtype = torch.long)\n",
    "      source_mask = source_mask.to(device, dtype = torch.float)\n",
    "      summary_ids = summary_ids.to(device, dtype = torch.long)\n",
    "\n",
    "      generatation = model.generate(input_ids=input_ids, \n",
    "                                    decoder_attention_mask=source_mask, \n",
    "                                    min_length=30, \n",
    "                                    max_length=300,                          \n",
    "                                    num_beams=3,\n",
    "                                    repetition_penalty=2.5,\n",
    "                                    eos_token_id = model.config.eos_token_id,               \n",
    "                                    length_penalty=1.0, \n",
    "                                    early_stopping=True\n",
    "                                    )\n",
    "\n",
    "      for sc in summary_ids: # Taking whole document IDs and docode the summary\n",
    "        #print(sc.size())\n",
    "        sc_tokens_dec = tokenizer.decode(sc, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        results[\"Original_summary\"].append(sc_tokens_dec)\n",
    "        \n",
    "      for gen in generatation:\n",
    "        gen_tokens_dec = tokenizer.decode(gen, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        results[\"Generated_summary\"].append(gen_tokens_dec)\n",
    "\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validation(model, val_loader, iteration=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples and summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: , this may be represented by a symbol representing a diode, an arrow pointing at a vertical line. ,, place the red probe on the positive side of the diode and the black probe on the negative side. if you get a reading of less than 1 but more than 0, the forward bias is good. , if the display reads ol overload , this indicates that reverse bias is good. , some multimeters will beep when the reading is less than 1. this doesn t necessarily indicate a good diode, because a shorted diode will also cause a beep. '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insert the black probe into the common terminal and the red probe into the terminal marked for measuring ohms, volts or diode test., use the selector knob to choose the test diode function., turn the power off to the circuit., test forward bias., reverse the probes to test reverse bias., a reading of ol or 0 while testing forward bias, and 0 while testing reverse bias indicates that the diode is bad.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', place the red probe on the positive side of the diode and the black probe on negative side., read forward bias., check for ol overload., beep when the reading is less than 0., try to get a good display., use a shorted diometer., test for reverse bias., see if the display shows oh overload., check for ol overload., check for ol overload., check for ol overload., check for ole overload., find out how long it takes to measure the output voltage., turn off the display., set up an alarm., switch off the display., change the display., reset the display., adjust the display., select the display., choose the display., put the display., remove the display., repeat this step by step with your multimeter., stop the display., restart the display., start over again., enter the display., then click on the display., taping in the display., wait until you get a second timers that are not working., close the display., take the display., run the display., go backward-brightly position the display., look at the display., watch the display., follow the display., turn the display., turn the display., turn the'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: if you are in an altercation, reaching out to an authority figure can keep you out of trouble. schedule an appointment with your counselor and ask them for advice. you can explain your situation using fake names if that helps you feel more comfortable about opening up. you can reveal as much or as little as you choose. overall, your counselor will just want to make sure that you are safe., there is no shame in asking your peers for help if you are in trouble. call a friend that you trust. a friend can also have your back if you need them during the fight. come up with a signal word between the two of you so that your friend will know right away if you re in trouble., if you are anticipating a fight and hope to avoid it, walk around with a large group of people or with a trustworthy friend. if you outnumber the bully or if there is a large audience, he or she will feel less emboldened to threaten or intimidate you., trusted adults can give you helpful advice on how to handle a bully or how to evade a fight. ask an adult you trust if they have ever gotten into a fight, and ask how they would handle the situation if they were in your shoes. coaches, parents, and other mentors can give you tips on the best ways to defend yourself and won t judge you if you admit to feeling scared. keep your life stress free and get help from an adult who cares about your wellbeing.remember that it s takes strength and bravery to try to avoid a fight altogether. '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[1,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk to a counselor., ask a friend for assistance., travel in groups or with a friend., talk to parents, a coach, or a mentor.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['article'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if you are in trouble, reach out to your counselor., ask for help., talk with friends., walk around., get help from trusted adults., avoid a fight altogether., get help from an adult who cares about your wellbeing and wants to help you., find therapists or other support groups., seek help from someone else., be prepared., try to avoid threatening situations., make sure that you are safe., don't feel ashamed., do not give up., stay calm., keep yourself\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['summary'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: marshmallows are very sticky, so you need to start with a greased pan. spray down a 9 by 13 inch pan 23 centimeters by 33 centimeters with cooking spray. it can also help to spray down any utensils you re going to use. the mixture will stick to anything., mix the packets of gelatin with about half a cup 125 milliliters of water and the vanilla extract in the bowl of a stand mixer. just use a fork to mix them together. let the mixture sit to the side for a few minutes so the gelatin can bloom hydrate .try to get all the lumps out with the fork.\\nyou can also use extracts like raspberry, lemon, orange, almond, or banana, depending on what you want to do with the marshmallows. , add 1 2 cup 125 milliliters of water to a small pan. pour in the sugar or the sugar plus the corn syrup, whichever you re using. bring it to a boil over medium to high heat, and let it boil until it reaches 250 degrees fahrenheit 121 degrees celsius or just under. continually wipe down the insides of the pan with a pastry brush to keep sugar from crystallizing there.alternatively, you can let the sugar and water come to a boil, and take it off the heat.corn syrup helps keep the mixture from crystallizing, meaning your marshmallows will usually stay softer. , you have two options here. if you raised the mixture to 250 degrees, simply pour it into the mixing bowl with the gelatin. do it slowly, and have the mixer on low.alternatively, if you just heated the mixture to boiling, add the gelatin to the pot. heat it again until the mixture is boiling, stirring as it comes to a boil. pour the mixture into the stand mixer, and let it cool down until it is just warm before going to the next step.if you prefer not to use gelatin, you can try agar in a one to one ratio with gelatin. another option is vegan jel.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[10,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grease the pan., hydrate the gelatin., make the simple syrup., mix the gelatin in.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['article'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a greased pan is best., add the gelatin and vanilla extract to melted butter., heat the mixture over medium-high heat., stir in the corn syrup., bring it to 125 degrees., raise the temperature of the mixture to 250 degrees., add the gelatin into the mixing bowl., let it cool down., add the gelatin to the pot., add the corn syrup., make sure you have enough water for your marshmallows., set aside., pour the gelatin into the saucepan., mix together the gelatin., boil the mixture., cook until the gelatin., put the gelatin on low heat., turn off the heat., remove the gelatin., cover with agar., use agar., place the gelatin., sprinkle some baking powder., bake them., roasting soda or sugar., simmer again., serve., cut out the gelatin., prepare the gelatin., allow the gelatin., beat well., then add the gelatin., combine all ingredients., reduce the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add the gelatin., add'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['summary'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KX71gPzy_bH"
   },
   "source": [
    "######Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ter9ktdQ6_54"
   },
   "outputs": [],
   "source": [
    "def metric(labels, gen_target):\n",
    "  print(f'Start evaluation')\n",
    "  bar = progressbar.ProgressBar(maxval=len(dataset_loader))\n",
    "  bar.start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  bar.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTkqfrwoy5QR"
   },
   "source": [
    "##### Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDjdxQUI6_1w"
   },
   "outputs": [],
   "source": [
    "model.decoder.block[11].requires_grad=True\n",
    "model.decoder.block[4].requires_grad=True\n",
    "\n",
    "for param model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfdlo_5_6_xS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqWc1IdF6_tA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7FyuTjN6_om"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2O8oTENj6_kI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKLE0jCV6_Wu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IElLmIW36_Sx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Be5LE3q6_O2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imlmpMSC6_K2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CLDteX96_Gg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOSEmtEe6_Ap"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCbkYzyj6-7M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x15lYdFGz0XL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NrQfhPSz0UQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYnfvPt3z0Rf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OljFlfM5z0OW"
   },
   "outputs": [],
   "source": [
    "class T5Model(nn.Module):\n",
    "\n",
    "  def __init__(self, t5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkpccxfe_Rwc"
   },
   "outputs": [],
   "source": [
    "class DictionaryDataset:\n",
    "  def __init__(self, dataset=None):\n",
    "    self.w2index = defaultdict(int)\n",
    "    self.index2w = defaultdict(lambda: 'SOS')\n",
    "    self.symbols = ['SOS','EOS','PAD']\n",
    "    self.wcount = 3\n",
    "\n",
    "    for i, s in enumerate(self.symbols):\n",
    "      self.index2w[i] = s\n",
    "    for i, s in enumerate(self.symbols):\n",
    "      self.w2index[s] = i\n",
    "      \n",
    "    if dataset:\n",
    "      for i, line in tqdm(enumerate(dataset)):\n",
    "        self.word2index(line)\n",
    "\n",
    "  def word2index(self, sentence):\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "      if word not in self.w2index.keys():\n",
    "        self.w2index[word] = self.wcount\n",
    "        self.index2w[self.wcount] = word\n",
    "        self.wcount +=1\n",
    "    \n",
    "  def encode_sequence(self, dataset):\n",
    "    for i, line in tqdm(enumerate(dataset)):\n",
    "      self.word2index(line)\n",
    "\n",
    "  def decode_idx(self, idx):\n",
    "    dec_words = []\n",
    "    for i in idx:\n",
    "      dec_words.append(self.index2w[i])\n",
    "    return dec_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h01oJ3XRCSBV"
   },
   "source": [
    "![LSTM](https://drive.google.com/open?id=19pYBGaKGx2wTHjDC3QLMkpr5HDqTPbb6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujAMkc2nDPaB"
   },
   "source": [
    "![Seq2SeqWithAttention](https://drive.google.com/open?id=1QDLNN0XQ0TfeA-nyAGBhG3YgwAX0bPQU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfvg8MT3RBFI"
   },
   "source": [
    "You should use the same number of layers, for encoder and decoder\n",
    "shape for inputs is (batch size, sequence length), (batch size, 1) respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQF-TwabDk4Q"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,input_size, embed_size, num_hidden, batch_size, num_layers=3, dropout = 0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.num_hidden = num_hidden\n",
    "    self.num_layers = num_layers\n",
    "    self.input_size = input_size\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.embed = nn.Embedding(input_size, embed_size)\n",
    "    self.lstm = nn.LSTM(input_size=embed_size, hidden_size=num_hidden,num_layers = num_layers, dropout=(0 if num_layers ==2 else dropout))\n",
    "\n",
    "  def init_hidden(self):\n",
    "     # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "    return (torch.zeros(self.num_layers, self.batch_size, self.num_hidden), torch.zeros(self.num_layers, self.batch_size, self.num_hidden))\n",
    "  def forward(self, input, hidden):\n",
    "    # input (batch size, sequence length)\n",
    "    embedding = self.embed(input)\n",
    "    #print(embedding.shape)\n",
    "    embed_out = embedding.transpose(0,1)\n",
    "    print(embed_out.shape)\n",
    "    lstm_out, hidden = self.lstm(embed_out, hidden)\n",
    "\n",
    "    # The shape of lstm_out is (seq_len, batch_size, num_hiddens).\n",
    "    # state contains the hidden state and the memory cell\n",
    "    # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n",
    "    #lstout is (seq_length, batch_size, num_hiddens)\n",
    "    return lstm_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m4_h0bXp0cl7",
    "outputId": "1310d376-e495-41c8-fd80-730b4970dca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size =10, embed_size=8, num_hidden=16, batch_size=4)\n",
    "X = torch.zeros((4, 7),dtype=torch.long)\n",
    "hidden = encoder.init_hidden()\n",
    "output, hidden = encoder(X, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zJ53T62V6ZOO",
    "outputId": "70b9e373-d6ba-4eed-da7d-b002257ab62b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 16])"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0XNO21wuBTGi",
    "outputId": "ab86113b-05af-4271-d479-df115a533fd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e55gItUrF_wD"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, hidden_size, method='dot'):\n",
    "    super(Attention, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "  def dot_score(self, hidden, encoder_output):\n",
    "    return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "  def forward(self, hidden, encoder_output):\n",
    "    att_prod = self.dot_score(hidden, encoder_output)\n",
    "    attn_weights = att_prod.t()\n",
    "    return F.softmax(attn_weights, dim=1).unsqueeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lp5VhnhRnuaL"
   },
   "outputs": [],
   "source": [
    "class DecoderWithAtt(nn.Module):\n",
    "  def __init__(self, embed_size_in, num_hidden, output_size, att_method=None, num_layers = 3):\n",
    "    super(DecoderWithAtt, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.num_hidden = num_hidden\n",
    "    self.output_size = output_size\n",
    "    self.att_method = att_method\n",
    "\n",
    "    self.emb = nn.Embedding(embed_size_in, num_hidden)\n",
    "    self.lstm = nn.LSTM(input_size=num_hidden, hidden_size=num_hidden, num_layers = num_layers)\n",
    "    self.con_ln = nn.Linear(num_hidden * 2, num_hidden)\n",
    "    self.drop = nn.Dropout(p=0.2)\n",
    "    self.fc  = nn.Linear(self.num_hidden, self.output_size)\n",
    "    self.attention = Attention(att_method, num_hidden)\n",
    "\n",
    "  def forward(self, prev_hidden, target_input, enconder_outputs):\n",
    "    embed_out = self.emb(target_input).transpose(0,1)\n",
    "    embed_out = self.drop(embed_out)\n",
    "    print('embed out >>>', embed_out.shape)\n",
    "    output, hidden = self.lstm(embed_out, prev_hidden) # shape (1, batch_size, hidden_size), 1 for one time step word \n",
    "    print('lstm out >>>', output.shape)\n",
    "    print('hidden out >>>', hidden[0].shape)\n",
    "\n",
    "    attent_weights = self.attention(output, enconder_outputs) # (att out shape: batch_size, 1, max_length), enc in shape: (max_length, batch_size, hidden_size)\n",
    "    print('attent_weights out >>>', attent_weights.shape)\n",
    "    print('enconder_outputs >>>', enconder_outputs.shape)\n",
    "\n",
    "    context = torch.bmm(attent_weights, enconder_outputs.transpose(0,1))\n",
    "    print('context out >>>', context.shape)\n",
    "    cont_dec_out = torch.cat((context, output.transpose(0,1)), 1)\n",
    "    print('to linear out >>>', cont_dec_out.shape)\n",
    "    cont_dec_out = cont_dec_out.view(cont_dec_out.shape[0], -1)\n",
    "    concat_out = self.con_ln(cont_dec_out)\n",
    "    concat_out = torch.tanh(concat_out)\n",
    "\n",
    "    out = self.fc(concat_out)\n",
    "    out = F.softmax(out, dim=1)\n",
    "\n",
    "    return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--9oa_EJ6_GG"
   },
   "outputs": [],
   "source": [
    "decoder = DecoderWithAtt(10,16,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "KHklHQXG7x4d",
    "outputId": "eeeb0cd9-0cd2-424f-dc6a-3823273e7fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed out >>> torch.Size([1, 4, 16])\n",
      "lstm out >>> torch.Size([1, 4, 16])\n",
      "hidden out >>> torch.Size([3, 4, 16])\n",
      "attent_weights out >>> torch.Size([4, 1, 7])\n",
      "enconder_outputs >>> torch.Size([7, 4, 16])\n",
      "context out >>> torch.Size([4, 1, 16])\n",
      "to linear out >>> torch.Size([4, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((4, 1),dtype=torch.long)\n",
    "out, hidden = decoder(hidden, x, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oW94xBGOOPQH",
    "outputId": "0cc0b4a1-9b8f-4a40-db3a-0a809f844f35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fSB0L7_6QyEN",
    "outputId": "6b4d1c40-e5b1-446d-c843-4deb13218d54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 16])"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Rl0M8_pQ6sD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPImDTXuNXNAukq7YpvmS64",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "T5-ftransfer_learning",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "54ac741b33df4657a497c5a20990d260": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e176b55e3d44509bf08563d814a22cc",
       "IPY_MODEL_75a74d040dd0496d8dadbe91152a4364"
      ],
      "layout": "IPY_MODEL_5662a03be19444fab76619ad47449786"
     }
    },
    "5662a03be19444fab76619ad47449786": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6630ef87ba4b471caf23a0e368ceb791": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75a74d040dd0496d8dadbe91152a4364": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89f558dbc0694ff489eb6b77400cb5c3",
      "placeholder": "​",
      "style": "IPY_MODEL_6630ef87ba4b471caf23a0e368ceb791",
      "value": " 4/? [00:05&lt;00:00,  1.30s/it]"
     }
    },
    "89f558dbc0694ff489eb6b77400cb5c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e176b55e3d44509bf08563d814a22cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6379aa1f5724141a274ff17c2e82f01",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e293b28a9f364f75ace7642678abd828",
      "value": 1
     }
    },
    "a6379aa1f5724141a274ff17c2e82f01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e293b28a9f364f75ace7642678abd828": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
